{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cafcdb2240b44f4486a0cf5e6157b105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "tqdm_notebook().pandas(leave=False)\n",
    "\n",
    "\n",
    "import spacy\n",
    "import en_core_web_md\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "from spacy_lookup import Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABUSE       = 'a'\n",
    "CONSUMPTION = 'c'\n",
    "MENTION     = 'm'\n",
    "UNRELATED   = 'u'\n",
    "\n",
    "classmap = {\n",
    "    'ABUSE'       : 'a',\n",
    "    'CONSUMPTION' : 'c',\n",
    "    'MENTION'     : 'm',\n",
    "    'UNRELATED'   : 'u'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2>Load tweet data, along with docs with drugnames and phrases </h2>\n",
    "Uncomment instances of fid_eval and df_eval if you have the final task evaluation data<br>\n",
    "Also fid_pred and df_pred are predictions on the eval data from an upstream model, uncomment if you have such data and want to see which samples are overwritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "\n",
    "fid_train = 'train.csv'\n",
    "fid_val = 'validation.csv'\n",
    "fid_eval = 'task4_test_participant.csv'\n",
    "fid_pred = 'prediction_task4.csv'\n",
    "\n",
    "fid_drugs = 'drugs_list - Sheet1.csv'\n",
    "fid_keywords =  'word_list - Sheet1.csv'\n",
    "fid_expressions = 'expression_list - Sheet1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper funcs\n",
    "\n",
    "def load_tweets(fid, rename_cols={}):\n",
    "    df = pd.read_csv(fid).rename(columns=rename_cols)\n",
    "    df['class'] = df['class'].map(lambda x: x.lower().strip()) if 'class' in df.columns else None\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tweet data\n",
    "\n",
    "# df_train = load_tweets(fid_train, {'unprocessed_text':'text'})\n",
    "# df_val   = load_tweets(fid_val  , {'unprocessed_text':'text'})\n",
    "# df_eval  = load_tweets(fid_eval , {'Tweet':'text'})\n",
    "# df_pred  = load_tweets(fid_pred , {'Class':'P'}).merge(df_eval.drop(columns=['class']), how='outer', on='tweetid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load other docs used in our rules\n",
    "\n",
    "df_drugs = pd.read_csv(fid_drugs)\n",
    "# make sure we fill Nan values for indeicator columns with False\n",
    "df_drugs = df_drugs.applymap(lambda x: x if not pd.isnull(x) else False)\n",
    "# we add extra rows for plural versions of drug\n",
    "df_drugs = pd.concat([df_drugs,\n",
    "                      df_drugs.assign(term = lambda x: x['term'] + 's')],\n",
    "                     axis=0)\n",
    "\n",
    "\n",
    "# create an nlp pipe that will search for each drug term in our list\n",
    "# we also create a LUT we can use to get more info about an identified drug in a tweet\n",
    "drug_dict = df_drugs.set_index('term').to_dict(orient='index')\n",
    "drug_entity_pipe = Entity(keywords_list=list(drug_dict.keys()), label='DRUG')\n",
    "# in case we rerun the cell, need tofirst remove pipe before re-adding\n",
    "if 'entity' in nlp.pipe_names:\n",
    "    nlp.remove_pipe('entity')\n",
    "nlp.add_pipe(drug_entity_pipe, before='ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists of different verbs associated with each class\n",
    "# NOTE: we only have ABUSE and CONSUMPTION verbs in our document right now\n",
    "df_words = pd.read_csv(fid_keywords)\n",
    "verbs_slang       = df_words[(df_words['term_type'].str.upper()=='VERB')&(df_words['class'].str.upper()=='ABUSE')].term.to_list()\n",
    "verbs_consumption = df_words[(df_words['term_type'].str.upper()=='VERB')&(df_words['class'].str.upper()=='CONSUMPTION')].term.to_list()\n",
    "\n",
    "\n",
    "# create lists of expressions assoiated with each class\n",
    "df_expr = pd.read_csv(fid_expressions)\n",
    "abuse_expressions       = df_expr[df_expr['class'].str.upper()=='ABUSE'].regex.to_list()\n",
    "mention_expressions     = df_expr[df_expr['class'].str.upper()=='MENTION'].regex.to_list()\n",
    "consumption_expressions = df_expr[df_expr['class'].str.upper()=='CONSUMPTION'].regex.to_list()\n",
    "unrelated_expressions   = df_expr[df_expr['class'].str.upper()=='UNRELATED'].regex.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2>We can now create a preprocessing pipe that tag tweets with info about found drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create functions for tagging docs with info on found drugs\n",
    "has_drug_term = lambda doc : any([True for ent in doc.ents if ent.label_ == 'DRUG'])\n",
    "has_slang_drug_term = lambda doc : any([ent._.get('is_slang') for ent in doc.ents if ent.label_ == 'DRUG'])\n",
    "has_commonly_abused_drug_term = lambda doc : any([ent._.get('is_commonly_abused') for ent in doc.ents if ent.label_ == 'DRUG'])\n",
    "\n",
    "\n",
    "def nlpify(text, lower=False):\n",
    "    # returns doc object with 'DRUG' entity and flags to indicate info about found drugs\n",
    "    text = text.lower() if lower else text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    Span.set_extension('is_commonly_abused', default=False, force=True)\n",
    "    Span.set_extension('is_slang', default=False, force=True)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'DRUG':\n",
    "            ent._.set('is_commonly_abused', drug_dict[ent.text.lower()]['is_commonly_abused'])\n",
    "            ent._.set('is_slang', drug_dict[ent.text.lower()]['is_slang'])\n",
    "    Doc.set_extension('has_drug_term', getter=has_drug_term, force=True)\n",
    "    Doc.set_extension('has_commonly_abused_drug_term', getter=has_commonly_abused_drug_term, force=True)\n",
    "    Doc.set_extension('has_slang_drug_term', getter=has_slang_drug_term, force=True)\n",
    "    return doc\n",
    "\n",
    "\n",
    "# make a helper func to apply the above method to our dataframes\n",
    "_nlpify = lambda row: nlpify(row['text'],lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's apply the proprocessing to our data\n",
    "# df_train['doc'] = df_train.progress_apply(_nlpify,  axis=1)\n",
    "# df_val['doc'] = df_val.progress_apply(_nlpify,  axis=1)\n",
    "# df_eval['doc'] = df_eval.progress_apply(_nlpify,  axis=1) #NOTE: dont really need eval since we have pred (eval+predictions)\n",
    "# df_pred['doc'] = df_pred.progress_apply(_nlpify,  axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2>Define Override Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retweet(doc, P, score=None, threshold=1,  pass_p=True):\n",
    "    '''\n",
    "    We check if this is a retweet\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_songs(doc, P, score=None, threshold=1, pass_p=True):\n",
    "    '''\n",
    "    phrases from commonly referenced songs about drugs\n",
    "    '''\n",
    "    PASS = P if pass_p else None\n",
    "    if re.search('(camila[ ]?cabello|camilzer|kcamexico|(nicotine, heroin, morphine)|(nicotine, valium, vicodin)|qotsa)', doc.text):\n",
    "        return MENTION\n",
    "    else:\n",
    "        return PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pet_meds(doc, P, score=None, threshold=1, pass_p=True):\n",
    "    '''\n",
    "    looks for a phrase to indicate the drug being mentioned is used by a pet\n",
    "    e.g. \"doggie valium\", or \"cat xanax\"\n",
    "    '''\n",
    "    PASS = P if pass_p else None\n",
    "    \n",
    "    drug_ents = [ent for ent in doc.ents if ent.label_=='DRUG']\n",
    "    for ent in drug_ents:\n",
    "        if re.search('(pup(py|per)?|dog(go|gie)?|cat|kit(ten|ty)) $', doc.text[:ent.start_char]):\n",
    "            return MENTION\n",
    "    else:\n",
    "        return PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_dose(doc, P, score=None, threshold=1, pass_p=True):\n",
    "    '''\n",
    "    look for phrases like \"took a double-dose of <drug>\"\n",
    "    indicates ABUSE\n",
    "    '''\n",
    "    PASS = P if pass_p else None\n",
    "    \n",
    "    if re.search('i (took|had)( (a|an))? (double|extra)( of (the|my))?[ -](dose|dosage|pill)', doc.text):\n",
    "        return ABUSE\n",
    "    else:\n",
    "        return PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lyrica_anderson(doc, P, score=None, threshold=1, pass_p=True):\n",
    "    '''\n",
    "    search for terms that are likely to be associated with Lyrica the artist\n",
    "    \n",
    "    LOGIC:\n",
    "    - if is the word lyrica mentioned in tweet\n",
    "        - if the number of unique possible drug mentions is 1 (e.g.lyrica)\n",
    "            - check that we find at least one term in the tweet associated with the artist\n",
    "            - check that we do not find a regex pattern that matches dose information for a drug\n",
    "            - if we meet the above 2 criteria\n",
    "                - override P and return UNRELATED\n",
    "              else \n",
    "                - return P\n",
    "          else\n",
    "            - if P is UNRELATED (even though we see another likely drug term)\n",
    "                - override P and return MENTION\n",
    "              else \n",
    "                - return P\n",
    "      else\n",
    "        - return P\n",
    "        \n",
    "    '''\n",
    "    PASS = P if pass_p else None\n",
    "    \n",
    "    person_terms = {'anderson', 'lhhh', 'lgbt', 'lgbtq',\n",
    "                    'song', 'album', 'sing', 'sang', 'singing', 'record',\n",
    "                    'hip hop', 'hiphop', 'teairra', 'moniece', 'masika', \n",
    "                    'safaree', 'omarion',}\n",
    "    \n",
    "    drug_terms = {'prescription(s)?', 'fybromyalgia', 'diabet(es|ic)',\n",
    "                  'shingles', 'medicat(ion(s)?|ed)', 'seizure(s)?',\n",
    "                  'pharmac(y|ist)',\n",
    "                  r'\\d+[ ]?(mg|mcg|ml|mcl|pills|tablet(s)?)'}\n",
    "    drug_rex = '(' + '|'.join(drug_terms) +  ')'\n",
    "     \n",
    "    # pass if we dont see lyrica in the tweet\n",
    "    found_drugs = [ent.text for ent in doc.ents if ent.label_=='DRUG']\n",
    "    if 'lyrica' not in found_drugs:\n",
    "        return PASS\n",
    "    # if lyrica in the tweet, but there's another possible drug, we would rather pass\n",
    "    elif len(set(found_drugs))>1:\n",
    "        # however, if the model predicts unrelated we may want to override\n",
    "        # with some more likely class, like MENTION since a second possible drug is not likely to be referring to a person\n",
    "        if P==UNRELATED:\n",
    "            return MENTION\n",
    "        else:\n",
    "            return PASS\n",
    "    \n",
    "    # check if we see any key terms we know are associated with the person\n",
    "    # also check that the text does not match a typical dose string pattern.\n",
    "    # if we pass both criteria  we will overrise and return UNRELATED, otherwise pass the original pred\n",
    "    tokens = set([t.text for t in  doc])\n",
    "    if len(tokens.intersection(person_terms))>0 and not re.search(drug_rex, doc.text):\n",
    "        return UNRELATED\n",
    "    else:\n",
    "        return PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drug_hadme(doc_raw, P, score=None, threshold=1, pass_p=True):\n",
    "    '''\n",
    "    We check for a phrase similar to \"<DRUG> had/got/made me/you\"\n",
    "    \n",
    "    LOGIC:\n",
    "    - check for the phrase\n",
    "        - If phrase found\n",
    "            - check that the preceding term is a possible drug\n",
    "                - if matches pattern\n",
    "                    - check if proceeding text doesnt include abuse terms\n",
    "                        - if abuse terms found\n",
    "                            - override P and return ABUSE (?)\n",
    "                          else\n",
    "                            - override P and return CONSUMPTION\n",
    "                  else\n",
    "                    - return P\n",
    "          else\n",
    "            - return P\n",
    "            \n",
    "    LOGIC (DEP):\n",
    "    - if root in accepted lemmas (e.g. root had/got/made)\n",
    "        - merge noun phrases together (to simplify checking drug term dependency)\n",
    "            - if root still acceptable lemma after merge\n",
    "                - check that we have at least root-child that is a noun phrase with a drug term in it\n",
    "                    - if we found at least one\n",
    "                        - check that we have at least one root-child that is a verb and a complement to the root verb\n",
    "                            - if we found at least one\n",
    "                                - check that for at least one of these verbs, there is a child that is a nsubj matching the target nouns (e.g. i,me,you,us)\n",
    "                                    - if we found at least one\n",
    "                                        - if we found more than one drug or an abuse-related verb above\n",
    "                                            - override p and return ABUSE\n",
    "                                          else\n",
    "                                            - override P and return CONSUMPTION\n",
    "      else\n",
    "        - return P\n",
    "    '''\n",
    "    PASS = P if pass_p else None\n",
    "    \n",
    "    # make a copy of doc since we modify it\n",
    "    doc = Doc(doc_raw.vocab).from_bytes(doc_raw.to_bytes())\n",
    "    \n",
    "#     print(doc.text)\n",
    "    ok_drugs = {'adderall', 'lisdexamfetamine', 'oxycodone', 'hydrocodone', 'alprazolam', 'percocet'}\n",
    "    ok_roots = {'get', 'have', 'make'}\n",
    "    abuse_verbs = {'trip', 'roll', 'rollin', 'hallucinate'}\n",
    "    ok_nouns = {'i', 'me', 'us'}\n",
    "\n",
    "    found_match = False\n",
    "    abuse_match = False\n",
    "    found_items = []\n",
    "    # first loop used to merge nsubj children for each root\n",
    "    for sent in doc.sents:\n",
    "        #if there are multiple sentences we need to split it, but only do so if we needto (otherwise is slow)\n",
    "        if len(list(doc.sents))>1:\n",
    "            sentdoc = nlpify(sent.text) # make a new doc that we can modify\n",
    "        else:\n",
    "            sentdoc = doc\n",
    "        roots = [token for token in sentdoc if token.head == token and token.lemma_ in ok_roots]\n",
    "        for root in roots:\n",
    "            nsubjs = [t for t in root.children if t.dep_=='nsubj']\n",
    "            mergespans = [sentdoc[t.left_edge.i : t.right_edge.i+1] for t in nsubjs]\n",
    "\n",
    "            with sentdoc.retokenize() as retokenizer:\n",
    "                [retokenizer.merge(span) for span in mergespans]\n",
    "            # second loop goes through each merged nsubj and checks it has a drug term\n",
    "            # also checks that there is a verb operated on by our root\n",
    "            # and that the children of the verb include a nounsubject matching i,you,me,use\n",
    "            roots = [token for token in sentdoc if token.head == token and token.lemma_ in ok_roots]\n",
    "            for root in roots:\n",
    "                # check that we find at least one nsubj with a drug term\n",
    "                nsubjs = [t for t in root.children if t.dep_=='nsubj']\n",
    "                drug_found = False\n",
    "                multidrug_found = False\n",
    "                for n in nsubjs:\n",
    "                    _d = nlpify(n.text)\n",
    "                    numdrugs = len([e for e in _d.ents if e.label_=='DRUG' and drug_dict.get(e.text, {}).get('parent_term') in ok_drugs])\n",
    "                    if numdrugs>0:\n",
    "                        drug_found = True\n",
    "                        if numdrugs>1:\n",
    "                            multidrug_found = True\n",
    "                if drug_found:\n",
    "                    verbs = [t for t in root.children if t.pos_=='VERB']# and t.dep_ in ('xcomp', 'ccomp')]\n",
    "                    if len(verbs)>0:\n",
    "                        for verb in verbs:\n",
    "                            v_children = [t for t in verb.children if t.dep_=='nsubj' and t.text in ok_nouns]\n",
    "                            if len(v_children)==0:\n",
    "                                #check if the noun subject (me/us) is for some reason treated as dobj of root\n",
    "                                v_children = [t for t in root.children if t.dep_=='dobj' and t.text in ok_nouns]\n",
    "                            if len(v_children)>0:\n",
    "                                found_items += [root, verb]+v_children\n",
    "                                found_match = True\n",
    "                                if not abuse_match and (verb.lemma_ in abuse_verbs or multidrug_found):\n",
    "                                    abuse_match = True\n",
    "\n",
    "    #backup regex just to check for simple pattern\n",
    "    for ent in [e for e in doc_raw.ents if e.label_=='DRUG' and drug_dict.get(e.text, {}).get('parent_term') in ok_drugs]:\n",
    "#         print(doc_raw.text[ent.end_char+1:])\n",
    "        if re.search('^[ ]*(have|having|has|had|got|made|making) (me|i|us)', \n",
    "                     doc_raw.text[ent.end_char+1:]):\n",
    "#             print('regex match')\n",
    "            found_match = True\n",
    "         \n",
    "    \n",
    "    # hacky way to check if in rt. normally would get token start/end idx but these were changed when retokenizing\n",
    "    is_in_rt = False\n",
    "    has_rt = re.search(r'(\"|“|”)_U:.*?(\"|“|”)', doc.text)\n",
    "#     print('has_rt', has_rt)\n",
    "    if has_rt:\n",
    "        start, end = has_rt.span()\n",
    "        if set([i.text for i in found_items]).issubset(set(doc.text[start:end].split())):\n",
    "            is_in_rt = True\n",
    "        \n",
    "                                    \n",
    "    if found_match:\n",
    "        if is_in_rt:\n",
    "            return MENTION\n",
    "        elif abuse_match:\n",
    "            return ABUSE\n",
    "        else:\n",
    "            return CONSUMPTION\n",
    "    else:\n",
    "        return PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lil_xan(doc, P, score=None, threshold=1, pass_p=True):\n",
    "    '''\n",
    "    We check for any mentions of lil xan or lil peep.\n",
    "    \n",
    "    in some cases if we find a reference to non-rapper term \n",
    "    we will override the model if it predicts unrelated, \n",
    "    since we assume there is at least one real drug mention\n",
    "    \n",
    "    LOGIC:\n",
    "    - get prediction P from model\n",
    "    - We look for all mentioned of xan/xanax\n",
    "        - If all of those refer to the rapper\n",
    "            - check if there are references to other drugs\n",
    "                - if there are other non-xanax drug references\n",
    "                    - if P is UNRELATED\n",
    "                        - override P and return MENTION\n",
    "                      else\n",
    "                        - return P\n",
    "                  else\n",
    "                    - override P and return UNRELATED\n",
    "          else\n",
    "            - if P is UNRELATED\n",
    "                - override P and return MENTION\n",
    "              else\n",
    "                - return P\n",
    "    '''\n",
    "    PASS = P if pass_p else None\n",
    "    \n",
    "    # if we don't see any possible mention to lil xan/peep, return original pred\n",
    "    if not re.search(r\"li(l|l'|ttle)[ -](xan|xanax|xanex|peep)\", doc.text):\n",
    "        return PASS\n",
    "    \n",
    "    # otherwise, we know there's some ambiguous mention to xan, or we found lil peep\n",
    "    # for each possible DRUG mention\n",
    "    for ent in [ent for ent in doc.ents if ent.label_ == 'DRUG']:\n",
    "        if ent.text not in ('xan', 'xanex', 'xanax'):\n",
    "            # we found a non- xan-ambiguous drug term\n",
    "            \n",
    "            # we only override pred if says the tweet is unrelated\n",
    "            # or if we set a threshold and it's over the P score\n",
    "            if P == UNRELATED or (score is not None and score<threshold):\n",
    "                return MENTION \n",
    "            else:\n",
    "                return PASS\n",
    "          \n",
    "        else:\n",
    "            if not re.search(r\"li(l|l'|ttle)[ -]&\", doc.text[:ent.start_char]):\n",
    "                # we found xan type mention that doesnt seem to be the rapper\n",
    "                \n",
    "                # same as above, we override under certain conditions\n",
    "                if P == UNRELATED or (score is not None and score<threshold):\n",
    "                    return MENTION \n",
    "                else:\n",
    "                    return PASS\n",
    "            else:\n",
    "                # we see a xan mention of the rapper\n",
    "                continue\n",
    "    # if we go throug each possible DRUG ent and each is likely the rapper, then unrelated\n",
    "    return UNRELATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def airline_names(doc, P, score=None, threshold=1, pass_p=True):\n",
    "    '''\n",
    "    check for airlines with drug-like names\n",
    "    we only override if we feel confident all mentions are related to airline\n",
    "    \n",
    "    LOGIC:\n",
    "    - get all unique possible drug terms\n",
    "        - if there is only \"dolophine\"\n",
    "            - check that each mention matches a regex indicating the airline\n",
    "                - if there is any mention not matching\n",
    "                    - return P\n",
    "                  else\n",
    "                    - return UNRELATED\n",
    "          else\n",
    "            - return P  \n",
    "    '''\n",
    "    PASS = P if pass_p else None\n",
    "    \n",
    "    is_airline = True\n",
    "    drug_ents = set([ent for ent in doc.ents if ent.label_ == 'DRUG'])\n",
    "    \n",
    "    # if only dolophine term found, check that each instance matches the regex\n",
    "    if len(drug_ents)>0 and drug_ents.issubset({'dolophine', 'doliphine'}):\n",
    "        for ent in drug_ents:\n",
    "            if not any([re.search(r'fl(ew|y|ying|own)( (via|on))? $', doc.text[:ent.start_char]),\n",
    "                        re.search(r'^ air(way|line)?(s)?', doc.text[ent.end_char+1:])]):\n",
    "                # mention of dolophine not likely related to airline\n",
    "                is_airline = False\n",
    "                break\n",
    "    else:\n",
    "        is_airline = False\n",
    "        \n",
    "    if is_airline:\n",
    "        return UNRELATED\n",
    "    else:\n",
    "        return PASS      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2>Now we can apply the rules to the data and original prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ordered dict indicating priority of rule override\n",
    "# the final prediction for each tweet is the first rule in thislist\n",
    "# that overrides the original prediction.\n",
    "# If no rule overrides, then we use the original prediction labeel\n",
    "rules_odict = OrderedDict({\n",
    "    'rule_airline'     : airline_names,\n",
    "    'rule_lilxan'      : lil_xan,\n",
    "    'rule_lyrica'      : lyrica_anderson, \n",
    "    'rule_hadme'       : drug_hadme,\n",
    "    'rule_extradose'   : extra_dose,\n",
    "    'rule_petmeds'     : pet_meds,\n",
    "    'rule_commonsongs' : common_songs,\n",
    "})\n",
    "\n",
    "\n",
    "def apply_rules(df, rules_odict):\n",
    "    # apply rules to our data\n",
    "    # note we want to cascade rules so we set it so that if a rule\n",
    "    # doesnt override it returns None, making it easier to use bfill later\n",
    "    # add any necessary placeholder columns, used by rules\n",
    "    placeholder_cols = {'P', 'score'} - set(df.columns)\n",
    "    for col in placeholder_cols:\n",
    "        df[col] = None\n",
    "    for k,v in tqdm_notebook(rules_odict.items(), desc='applying rules', leave=True):\n",
    "        df[k] = df.progress_apply(lambda x: v(*x[['doc', 'P', 'score']], pass_p=False), axis=1)\n",
    "    # using the order of rules defined above, we use bfill to select the first\n",
    "    # rule that overrides the original prediction and use that value  and the final\n",
    "    # if no rule overrides, then use the original prediction\n",
    "    df['P_final'] = df[list(rules_odict.keys())+['P']].bfill(axis=1).iloc[:,0]\n",
    "    return df\n",
    "\n",
    "\n",
    "# df_train = apply_rules(df_train, rules_odict)\n",
    "# df_val   = apply_rules(df_val, rules_odict)\n",
    "# df_eval  = apply_rules(df_eval, rules_odict)\n",
    "# df_pred  = apply_rules(df_pred, rules_odict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2>We can review the samples whose prediction labels were modified by our override rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pchange(df):\n",
    "    _df = df[(df.P!=df.P_final)&(~pd.isnull(df.P_final))]\n",
    "    print(_df.shape[0])\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_pchange(df_pred)\n",
    "# df_pred.to_excel('pred_results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import f1_score\n",
    "# print(f1_score(df_valvotes['class'], df_valvotes['P_final'], average='macro'))\n",
    "# df_valvotes.groupby('correct')['class'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Run rules on prediction files from Izzy</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3271.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run eval data through preprocessor\n",
    "df_eval  = load_tweets(fid_eval , {'Tweet':'text'})\n",
    "df_eval['doc'] = df_eval.progress_apply(_nlpify,  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prep pred data to have rules applied\n",
    "\n",
    "# fid_pred_bert = 'FINAL_PREDICTION_BERT.csv'\n",
    "# fid_pred_503 = '503split-2020-06-04 01_10_49.626597-predictions.csv'\n",
    "fid_pred_final = 'task4.csv'\n",
    "\n",
    "def clean_pred_df(fid):\n",
    "    df = pd.read_csv(fid)\n",
    "    df['score'] = df[['MENTION', 'CONSUMPTION', 'ABUSE', 'UNRELATED']].max(axis=1)\n",
    "    df['P'] = df['prediction'].map(lambda x: classmap[x])\n",
    "    df = df.merge(df_eval[['tweetid', 'text', 'doc']], how='left', on='tweetid')\n",
    "    return df\n",
    "\n",
    "# df_pred_bert = clean_pred_df(fid_pred_bert)\n",
    "# df_pred_503 = clean_pred_df(fid_pred_503)\n",
    "df_pred_final = clean_pred_df(fid_pred_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8861e7c433654662a98835f104a5eaff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='applying rules', max=7.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3271.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3271.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3271.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3271.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3271.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3271.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3271.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# run data through rules\n",
    "\n",
    "# df_pred_bert = apply_rules(df_pred_bert, rules_odict)\n",
    "# df_pred_503 = apply_rules(df_pred_503, rules_odict)\n",
    "df_pred_final = apply_rules(df_pred_final, rules_odict)\n",
    "\n",
    "# save a full output and also submittable version for each model\n",
    "# df_pred_bert.to_csv('BERT_pred_override_fullresults.csv', index=False)\n",
    "# df_pred_bert[['tweetid','P_final']].rename(columns={'P_final':'Class'}).to_csv('BERT_pred_override.csv', index=False)\n",
    "\n",
    "# df_pred_503.to_csv('503split_pred_ful_results.csv', index=False)\n",
    "# df_pred_503[['tweetid','P_final']].rename(columns={'P_final':'Class'}).to_csv('503split_pred_override.csv', index=False)\n",
    "\n",
    "df_pred_final.to_csv('task4_override_full_results.csv', index=False)\n",
    "df_pred_final[['tweetid','P_final']].rename(columns={'P_final':'Class'}).to_csv('task4_override.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
